{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7549a17b-849f-45eb-9a1b-b85d450447e0",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Mix of libraries for data preprocessing, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882d5aa7-5837-4c0f-9b47-79bd7b9969d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 18:47:07.833147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import imagesize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c062a-9374-41a1-ac72-932d2b9b4702",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Here, we will be creating a dataframe with all our image paths (as strings), bloodcell type, and image dimension information. We will take a look at if our data is balanced or not with the bloodcell type counts and the image dimension counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63fb38-d092-4954-b8c2-d7488d87c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basophil',\n",
       " 'neutrophil',\n",
       " 'ig',\n",
       " 'monocyte',\n",
       " 'eosinophil',\n",
       " 'erythroblast',\n",
       " 'lymphocyte',\n",
       " 'platelet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset folder has 8 different folders, which represent 8 different bloodcells we will be classifying\n",
    "# Open up folder names and remove folder names that are not bloodcell types\n",
    "\n",
    "# Original kaggle dataset has images stored in each of the 8 folders, so we made a folder that contained all the images\n",
    "# so that it is easier to convert the images to numpy arrays later on\n",
    "\n",
    "bloodcells = os.listdir(\"bloodcells_dataset\")\n",
    "bloodcells = [x for x in bloodcells if x not in ['.DS_Store', 'All_Images']]\n",
    "\n",
    "bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63160923-03db-4c6d-9690-64e2ff43f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_df(folder_names):\n",
    "\n",
    "    '''\n",
    "    Outputs a dataframe for image paths (as strings), bloodcell type, and image dimension information.\n",
    "\n",
    "    Args:\n",
    "        1) folder_names (list): list of bloodcell type folders\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with all image paths, bloodcell types, and image dimensions\n",
    "    '''\n",
    "\n",
    "    # initialize empty list to store dataframes that contain image strings and bloodcell type\n",
    "    dfs = []\n",
    "\n",
    "    # loop through bloodcell types and store image paths and bloodcell categories\n",
    "    for i in range(len(folder_names)):\n",
    "\n",
    "        # jpg string paths\n",
    "        images = os.listdir('bloodcells_dataset/' + folder_names[i]) \n",
    "\n",
    "        # dataframe holding specific bloodcell type info (string path and type name)\n",
    "        df = pd.DataFrame(data = {'images': images, 'type': folder_names[i]})\n",
    "\n",
    "        # append dataframe to list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # combine all dataframes\n",
    "    all_data = pd.concat(dfs)\n",
    "\n",
    "    # Remove image paths that may have been accidentally copied or contain .DS_Store\n",
    "    all_data = all_data[all_data['images'].str.contains('.DS_Store') == False]\n",
    "    all_data = all_data[all_data['images'].str.contains('copy') == False]\n",
    "\n",
    "    # Convert bloodcell types to numbers for our model\n",
    "    le = LabelEncoder()\n",
    "    all_data['type_category'] = all_data['type'] # keep a copy of bloodcell types by name\n",
    "    all_data['type'] = le.fit_transform(all_data['type'])\n",
    "\n",
    "    # Store dimensions of image incase we find different dimensions \n",
    "    dimensions = pd.Series([imagesize.get('bloodcells_dataset/All_Images/' + x) for x in all_data['images']])\n",
    "    widths, heights = map(list, zip(*dimensions))\n",
    "    all_data['width'] = widths\n",
    "    all_data['height'] = heights\n",
    "\n",
    "    # Reset index \n",
    "    all_data = all_data.reset_index(drop = True)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d61789-b0a5-4b10-9339-866eb38e8a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>type</th>\n",
       "      <th>type_category</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA_689200.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>basophil</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA_883452.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>basophil</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA_382161.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>basophil</td>\n",
       "      <td>366</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA_175579.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>basophil</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA_775722.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>basophil</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>PLATELET_495918.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>platelet</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>PLATELET_897238.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>platelet</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>PLATELET_750430.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>platelet</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>PLATELET_810431.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>platelet</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17091</th>\n",
       "      <td>PLATELET_499850.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>platelet</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17092 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    images  type type_category  width  height\n",
       "0            BA_689200.jpg     0      basophil    360     363\n",
       "1            BA_883452.jpg     0      basophil    360     363\n",
       "2            BA_382161.jpg     0      basophil    366     369\n",
       "3            BA_175579.jpg     0      basophil    360     363\n",
       "4            BA_775722.jpg     0      basophil    360     363\n",
       "...                    ...   ...           ...    ...     ...\n",
       "17087  PLATELET_495918.jpg     7      platelet    360     363\n",
       "17088  PLATELET_897238.jpg     7      platelet    360     363\n",
       "17089  PLATELET_750430.jpg     7      platelet    360     363\n",
       "17090  PLATELET_810431.jpg     7      platelet    360     363\n",
       "17091  PLATELET_499850.jpg     7      platelet    360     363\n",
       "\n",
       "[17092 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = image_df(bloodcells)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a3e57-80ad-4791-8320-39692f45fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuals for count of height/width and count of each blood cell type\n",
    "# Maybe use function to plot\n",
    "\n",
    "# df[['height', 'width']].value_counts().reset_index(name = 'count') \n",
    "# df[['type_category']].value_counts().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46f4355-e4ed-423d-89d8-f4e31d2e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "\n",
    "    def __init__(self, data, sampling_method):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "    def sample_data(self, sampling_percent = 0.8):\n",
    "\n",
    "        df = self.data.copy()\n",
    "\n",
    "        category_counts = df[['type']].value_counts().reset_index(name = 'count')\n",
    "\n",
    "        train_dfs = []\n",
    "\n",
    "        if self.sampling_method == 'weighted':\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                if category_counts['count'][i] >= 2000:\n",
    "                    add_samples = df[df['type'] == type].sample(1500)\n",
    "                else: \n",
    "                    add_samples = df[df['type'] == type].sample(1000)\n",
    "\n",
    "                train_dfs.append(add_samples)\n",
    "            \n",
    "            train = pd.concat(train_dfs)\n",
    "\n",
    "            remaining_data = df[~df['images'].isin(train['images'])]\n",
    "    \n",
    "            validation = remaining_data.sample(int(len(remaining_data) / 2))\n",
    "        \n",
    "        elif self.sampling_method == 'proportional': \n",
    "\n",
    "            num_samples = int(sampling_percent * len(df))\n",
    "            \n",
    "            category_counts['prop'] = category_counts['count'] / len(df)\n",
    "            category_counts['train_samples'] = (category_counts['prop'] * num_samples).astype('int32')\n",
    "            category_counts['val_samples'] = (category_counts['train_samples'] * 0.2).astype('int32')\n",
    "            category_counts['train_samples'] = category_counts['train_samples'] - category_counts['val_samples']\n",
    "\n",
    "            val_dfs = []\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                samples = category_counts['train_samples'][i]\n",
    "            \n",
    "                add_samples = df[df['type'] == type].sample(samples)\n",
    "\n",
    "                train_dfs.append(add_samples)\n",
    "            \n",
    "            train = pd.concat(train_dfs)\n",
    "\n",
    "            remaining_data = df[~df['images'].isin(train['images'])]\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                samples = category_counts['val_samples'][i]\n",
    "            \n",
    "                add_samples = remaining_data[remaining_data['type'] == type].sample(samples)\n",
    "\n",
    "                val_dfs.append(add_samples)\n",
    "            \n",
    "            validation = pd.concat(val_dfs)\n",
    "\n",
    "        train = train.dropna(how = 'all')\n",
    "\n",
    "        float_cols = train.select_dtypes(np.number)\n",
    "\n",
    "        train[float_cols.columns] = float_cols.astype('int32')\n",
    "\n",
    "        test = remaining_data[~remaining_data['images'].isin(validation['images'])]\n",
    "\n",
    "        return train, validation, test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f24ebb-9579-4255-9ec1-0988dc5aeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampling = Sampling(df, 'weighted')\n",
    "proportional_sampling = Sampling(df, 'proportional')\n",
    "\n",
    "weighted_train, weighted_val, weighted_test = weighted_sampling.sample_data()\n",
    "prop_train, prop_val, prop_test = proportional_sampling.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68225d0e-9948-457f-ba14-dee6f4920d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Images:\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.file_names = (self.data)['images'].apply(lambda x: 'bloodcells_dataset/All_Images/' + x)\n",
    "\n",
    "        self.labels = self.data['type']\n",
    "\n",
    "    def load_image(self, file_name, resize):\n",
    "        \n",
    "        raw = tf.io.read_file(file_name)\n",
    "        \n",
    "        tensor = tf.io.decode_image(raw, expand_animations = False)\n",
    "        \n",
    "        tensor = tf.image.resize(tensor, size = [resize, resize])\n",
    "        \n",
    "        tensor = tf.cast(tensor, tf.float32) / 255.0\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def image_arrays_and_labels(self, resize = 32):\n",
    "\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(self.file_names)\n",
    "        \n",
    "      dataset = dataset.map(lambda file_name: self.load_image(file_name, resize))\n",
    "        \n",
    "      images = np.array(list(dataset))\n",
    "        \n",
    "      return images, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fbedfb7-47ac-4a9f-bb99-fabcd515af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 18:47:21.398152: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "weighted_train_image_df, weighted_val_image_df, weighted_test_image_df = Convert_Images(weighted_train), Convert_Images(weighted_val), Convert_Images(weighted_test) \n",
    "prop_train_image_df, prop_val_image_df, prop_test_image_df = Convert_Images(prop_train), Convert_Images(prop_val), Convert_Images(prop_test)\n",
    "\n",
    "\n",
    "\n",
    "resize_pixels = 32\n",
    "\n",
    "weighted_train_images, weighted_train_labels = weighted_train_image_df.image_arrays_and_labels(resize_pixels)\n",
    "weighted_val_images, weighted_val_labels = weighted_val_image_df.image_arrays_and_labels(resize_pixels)\n",
    "weighted_test_images, weighted_test_labels = weighted_test_image_df.image_arrays_and_labels(resize_pixels)\n",
    "prop_train_images, prop_train_labels = prop_train_image_df.image_arrays_and_labels(resize_pixels)\n",
    "prop_val_images, prop_val_labels = prop_val_image_df.image_arrays_and_labels(resize_pixels)\n",
    "prop_test_images, prop_test_labels = prop_test_image_df.image_arrays_and_labels(resize_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ea57916-e704-4f90-91af-205ba06661b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_train_images: 10000\n",
      "weighted_val_images: 3546\n",
      "weighted_test_images: 3546\n",
      "prop_train_images: 10938\n",
      "prop_val_images: 2731\n",
      "prop_test_images: 3423\n"
     ]
    }
   ],
   "source": [
    "print(f'weighted_train_images: {len(weighted_train_images)}')\n",
    "print(f'weighted_val_images: {len(weighted_val_images)}')\n",
    "print(f'weighted_test_images: {len(weighted_test_images)}')\n",
    "print(f'prop_train_images: {len(prop_train_images)}')\n",
    "print(f'prop_val_images: {len(prop_val_images)}')\n",
    "print(f'prop_test_images: {len(prop_test_images)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8435b02e-1541-4ce4-b183-efca096a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, val_data, val_test, test_data, test_labels, optimizer = 'adam', epochs = 5, batch_size = 64):\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, \n",
    "                        train_labels, \n",
    "                        validation_data = (val_data, val_test),\n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size)\n",
    "\n",
    "    predictions = (model.predict(test_data)).argmax(axis = 1)\n",
    "\n",
    "    test_accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "\n",
    "    return history, predictions, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed2467d-ce27-4f82-b321-a8968fb0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st of 3 models\n",
    "# Simple model\n",
    "# One for weighted sampling and proportional sampling\n",
    "\n",
    "# Simple Model - Weighted Sampling\n",
    "sm_w = models.Sequential(\n",
    "    \n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (resize_pixels, resize_pixels, 3)),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "\n",
    "        # flatten into 1d array\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Neural network\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "\n",
    "        layers.Dropout(rate = 0.2),\n",
    "        \n",
    "        # 8 different categories\n",
    "        layers.Dense(8) \n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "# Simple Model - Proportional Sampling\n",
    "sm_p = models.clone_model(sm_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9e053d-f9db-408d-9f45-48b4403ce54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 1.4411 - accuracy: 0.4444 - val_loss: 0.8733 - val_accuracy: 0.7236\n",
      "Epoch 2/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.8611 - accuracy: 0.6800 - val_loss: 0.7740 - val_accuracy: 0.7315\n",
      "Epoch 3/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.7301 - accuracy: 0.7298 - val_loss: 0.6677 - val_accuracy: 0.7645\n",
      "Epoch 4/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.6455 - accuracy: 0.7610 - val_loss: 0.4404 - val_accuracy: 0.8404\n",
      "Epoch 5/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.5562 - accuracy: 0.7914 - val_loss: 0.4811 - val_accuracy: 0.8206\n",
      "Epoch 6/25\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5308 - accuracy: 0.8044 - val_loss: 0.3865 - val_accuracy: 0.8598\n",
      "Epoch 7/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.4958 - accuracy: 0.8130 - val_loss: 0.4041 - val_accuracy: 0.8573\n",
      "Epoch 8/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.4704 - accuracy: 0.8223 - val_loss: 0.3707 - val_accuracy: 0.8708\n",
      "Epoch 9/25\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.4401 - accuracy: 0.8369 - val_loss: 0.3692 - val_accuracy: 0.8706\n",
      "Epoch 10/25\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.4204 - accuracy: 0.8404 - val_loss: 0.4324 - val_accuracy: 0.8407\n",
      "Epoch 11/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3855 - accuracy: 0.8586 - val_loss: 0.3359 - val_accuracy: 0.8790\n",
      "Epoch 12/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3733 - accuracy: 0.8629 - val_loss: 0.3181 - val_accuracy: 0.8869\n",
      "Epoch 13/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3470 - accuracy: 0.8720 - val_loss: 0.2788 - val_accuracy: 0.9098\n",
      "Epoch 14/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3361 - accuracy: 0.8748 - val_loss: 0.2778 - val_accuracy: 0.9041\n",
      "Epoch 15/25\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.3114 - accuracy: 0.8853 - val_loss: 0.2808 - val_accuracy: 0.8990\n",
      "Epoch 16/25\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.2956 - accuracy: 0.8907 - val_loss: 0.2802 - val_accuracy: 0.9047\n",
      "Epoch 17/25\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2866 - accuracy: 0.9002 - val_loss: 0.2851 - val_accuracy: 0.8982\n",
      "Epoch 18/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2732 - accuracy: 0.9016 - val_loss: 0.2513 - val_accuracy: 0.9143\n",
      "Epoch 19/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2603 - accuracy: 0.9082 - val_loss: 0.2356 - val_accuracy: 0.9185\n",
      "Epoch 20/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2410 - accuracy: 0.9123 - val_loss: 0.2740 - val_accuracy: 0.9126\n",
      "Epoch 21/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2461 - accuracy: 0.9121 - val_loss: 0.2395 - val_accuracy: 0.9162\n",
      "Epoch 22/25\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 0.2321 - accuracy: 0.9183 - val_loss: 0.2345 - val_accuracy: 0.9193\n",
      "Epoch 23/25\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2197 - accuracy: 0.9226 - val_loss: 0.2647 - val_accuracy: 0.9112\n",
      "Epoch 24/25\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2233 - accuracy: 0.9194 - val_loss: 0.2372 - val_accuracy: 0.9191\n",
      "Epoch 25/25\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2098 - accuracy: 0.9253 - val_loss: 0.2225 - val_accuracy: 0.9239\n",
      "111/111 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "sm_w_history, sm_w_predictions, sm_w_test_accuracy = train_model(sm_w,\n",
    "                                                                 weighted_train_images,\n",
    "                                                                 weighted_train_labels,\n",
    "                                                                 weighted_val_images,\n",
    "                                                                 weighted_val_labels,\n",
    "                                                                 weighted_test_images,\n",
    "                                                                 weighted_test_labels,\n",
    "                                                                 epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b30932-2847-44c0-8672-b33fb28e5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "171/171 [==============================] - 6s 30ms/step - loss: 1.4215 - accuracy: 0.4695 - val_loss: 0.9121 - val_accuracy: 0.7027\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.8745 - accuracy: 0.6796 - val_loss: 0.8008 - val_accuracy: 0.7089\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.7361 - accuracy: 0.7222 - val_loss: 0.6505 - val_accuracy: 0.7675\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.6459 - accuracy: 0.7550 - val_loss: 0.6168 - val_accuracy: 0.7741\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.5968 - accuracy: 0.7767 - val_loss: 0.4935 - val_accuracy: 0.8279\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.5279 - accuracy: 0.8016 - val_loss: 0.4848 - val_accuracy: 0.8228\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.4617 - accuracy: 0.8271 - val_loss: 0.4317 - val_accuracy: 0.8499\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.4303 - accuracy: 0.8403 - val_loss: 0.3728 - val_accuracy: 0.8605\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.4023 - accuracy: 0.8524 - val_loss: 0.3839 - val_accuracy: 0.8634\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.3809 - accuracy: 0.8592 - val_loss: 0.3362 - val_accuracy: 0.8777\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 5s 31ms/step - loss: 0.3557 - accuracy: 0.8694 - val_loss: 0.3251 - val_accuracy: 0.8762\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 5s 32ms/step - loss: 0.3336 - accuracy: 0.8794 - val_loss: 0.3287 - val_accuracy: 0.8795\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.3138 - accuracy: 0.8847 - val_loss: 0.3302 - val_accuracy: 0.8770\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.3091 - accuracy: 0.8864 - val_loss: 0.3843 - val_accuracy: 0.8627\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2898 - accuracy: 0.8959 - val_loss: 0.2781 - val_accuracy: 0.8997\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.2766 - accuracy: 0.8990 - val_loss: 0.2608 - val_accuracy: 0.9022\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2599 - accuracy: 0.9082 - val_loss: 0.2906 - val_accuracy: 0.8938\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2504 - accuracy: 0.9105 - val_loss: 0.2635 - val_accuracy: 0.9099\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2497 - accuracy: 0.9103 - val_loss: 0.2338 - val_accuracy: 0.9143\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.2360 - accuracy: 0.9179 - val_loss: 0.2451 - val_accuracy: 0.9132\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2196 - accuracy: 0.9195 - val_loss: 0.2371 - val_accuracy: 0.9147\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.2162 - accuracy: 0.9237 - val_loss: 0.2618 - val_accuracy: 0.9085\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 5s 29ms/step - loss: 0.2071 - accuracy: 0.9248 - val_loss: 0.2348 - val_accuracy: 0.9169\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2035 - accuracy: 0.9277 - val_loss: 0.2707 - val_accuracy: 0.9096\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 5s 30ms/step - loss: 0.2011 - accuracy: 0.9291 - val_loss: 0.2849 - val_accuracy: 0.9019\n",
      "107/107 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "sm_p_history, sm_p_predictions, sm_p_test_accuracy = train_model(sm_p,\n",
    "                                                                 prop_train_images,\n",
    "                                                                 prop_train_labels,\n",
    "                                                                 prop_val_images,\n",
    "                                                                 prop_val_labels,\n",
    "                                                                 prop_test_images,\n",
    "                                                                 prop_test_labels, \n",
    "                                                                 epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc3e39c-3ac7-4be4-8499-1f019e3fb762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9289340101522843, 0.8951212386795209)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_w_test_accuracy, sm_p_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de38383-3841-4fdc-a5b3-8b9f56f54116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "060a206c-c883-4027-a22e-738eda2a808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully-Connected Neural Network\n",
    "def image_reshape(X):\n",
    "    \"\"\"\n",
    "    Reshapes a 3d/4d array of images into a 2d array\n",
    "\n",
    "    Args:\n",
    "        x (numpy array): 3d/4d array of images\n",
    "\n",
    "    Returns:\n",
    "        Reshaped 2d array of images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find number of images (1st dimension of 2d array)\n",
    "    num_of_images = X.shape[0]\n",
    "\n",
    "    # second dimension of 2d array (product of all remaining dimensions)\n",
    "    resize = np.prod(X.shape[1:])\n",
    "\n",
    "    # return 2d array\n",
    "    return X.reshape(num_of_images, resize)\n",
    "    \n",
    "\n",
    "# fnn weighted\n",
    "fnn_w = tf.keras.models.Sequential([\n",
    "    layers.Dense(64, input_shape = (resize_pixels * resize_pixels * 3, ), activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "    \n",
    "    layers.Dense(64, activation='relu'),    \n",
    "    \n",
    "    layers.Dense(8) # 8 categories                 \n",
    "])\n",
    "\n",
    "# fnn proportional\n",
    "fnn_p = models.clone_model(fnn_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c2812d6-344e-48b8-98f6-3e8e6bca85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "157/157 [==============================] - 2s 8ms/step - loss: 1.6295 - accuracy: 0.3630 - val_loss: 1.2322 - val_accuracy: 0.5601\n",
      "Epoch 2/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.1469 - accuracy: 0.5539 - val_loss: 0.9909 - val_accuracy: 0.6464\n",
      "Epoch 3/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.0370 - accuracy: 0.5973 - val_loss: 1.0420 - val_accuracy: 0.5558\n",
      "Epoch 4/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.9405 - accuracy: 0.6379 - val_loss: 1.1331 - val_accuracy: 0.5547\n",
      "Epoch 5/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.8959 - accuracy: 0.6552 - val_loss: 0.8994 - val_accuracy: 0.6540\n",
      "Epoch 6/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.8656 - accuracy: 0.6704 - val_loss: 0.8310 - val_accuracy: 0.6785\n",
      "Epoch 7/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.8349 - accuracy: 0.6846 - val_loss: 0.7657 - val_accuracy: 0.7036\n",
      "Epoch 8/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.8376 - accuracy: 0.6810 - val_loss: 0.8231 - val_accuracy: 0.6861\n",
      "Epoch 9/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.8015 - accuracy: 0.6957 - val_loss: 0.7615 - val_accuracy: 0.7121\n",
      "Epoch 10/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.7910 - accuracy: 0.6961 - val_loss: 1.0797 - val_accuracy: 0.6122\n",
      "Epoch 11/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.8041 - accuracy: 0.6978 - val_loss: 0.6983 - val_accuracy: 0.7406\n",
      "Epoch 12/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7745 - accuracy: 0.7016 - val_loss: 0.7096 - val_accuracy: 0.7208\n",
      "Epoch 13/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7358 - accuracy: 0.7201 - val_loss: 1.0025 - val_accuracy: 0.6435\n",
      "Epoch 14/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7757 - accuracy: 0.7033 - val_loss: 0.8693 - val_accuracy: 0.6737\n",
      "Epoch 15/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7223 - accuracy: 0.7239 - val_loss: 0.7727 - val_accuracy: 0.6974\n",
      "Epoch 16/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7410 - accuracy: 0.7211 - val_loss: 0.9027 - val_accuracy: 0.6560\n",
      "Epoch 17/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.6966 - accuracy: 0.7353 - val_loss: 0.9133 - val_accuracy: 0.6466\n",
      "Epoch 18/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.7070 - accuracy: 0.7350 - val_loss: 0.7366 - val_accuracy: 0.7084\n",
      "Epoch 19/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6759 - accuracy: 0.7450 - val_loss: 0.8248 - val_accuracy: 0.6884\n",
      "Epoch 20/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.7457 - val_loss: 0.8315 - val_accuracy: 0.6791\n",
      "Epoch 21/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.7539 - val_loss: 0.6332 - val_accuracy: 0.7541\n",
      "Epoch 22/25\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.7453 - val_loss: 0.5718 - val_accuracy: 0.7865\n",
      "Epoch 23/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.7664 - val_loss: 0.6621 - val_accuracy: 0.7504\n",
      "Epoch 24/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.7716 - val_loss: 0.5564 - val_accuracy: 0.7871\n",
      "Epoch 25/25\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.7700 - val_loss: 0.6289 - val_accuracy: 0.7594\n",
      "111/111 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "weighted_train_images_flat = image_reshape(weighted_train_images)\n",
    "weighted_val_images_flat = image_reshape(weighted_val_images)\n",
    "weighted_test_images_flat = image_reshape(weighted_test_images)\n",
    "\n",
    "fnn_w_history, fnn_w_predictions, fnn_w_test_accuracy = train_model(fnn_w,\n",
    "                                                                 weighted_train_images_flat,\n",
    "                                                                 weighted_train_labels,\n",
    "                                                                 weighted_val_images_flat,\n",
    "                                                                 weighted_val_labels,\n",
    "                                                                 weighted_test_images_flat,\n",
    "                                                                 weighted_test_labels, \n",
    "                                                                 epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a888394b-ff00-4d3c-9848-da204011458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "171/171 [==============================] - 1s 3ms/step - loss: 1.7251 - accuracy: 0.3470 - val_loss: 1.2879 - val_accuracy: 0.5445\n",
      "Epoch 2/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 1.1664 - accuracy: 0.5632 - val_loss: 1.0535 - val_accuracy: 0.5987\n",
      "Epoch 3/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.9909 - accuracy: 0.6247 - val_loss: 1.1156 - val_accuracy: 0.5339\n",
      "Epoch 4/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.9190 - accuracy: 0.6429 - val_loss: 0.8667 - val_accuracy: 0.6792\n",
      "Epoch 5/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8885 - accuracy: 0.6575 - val_loss: 0.8603 - val_accuracy: 0.6767\n",
      "Epoch 6/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8913 - accuracy: 0.6553 - val_loss: 0.8579 - val_accuracy: 0.6745\n",
      "Epoch 7/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8570 - accuracy: 0.6681 - val_loss: 0.8893 - val_accuracy: 0.6664\n",
      "Epoch 8/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8165 - accuracy: 0.6839 - val_loss: 0.8171 - val_accuracy: 0.6888\n",
      "Epoch 9/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.8217 - accuracy: 0.6865 - val_loss: 0.7971 - val_accuracy: 0.6983\n",
      "Epoch 10/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7833 - accuracy: 0.6933 - val_loss: 0.8237 - val_accuracy: 0.6975\n",
      "Epoch 11/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7885 - accuracy: 0.6982 - val_loss: 0.8219 - val_accuracy: 0.6866\n",
      "Epoch 12/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7759 - accuracy: 0.7011 - val_loss: 0.7891 - val_accuracy: 0.7027\n",
      "Epoch 13/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7563 - accuracy: 0.7059 - val_loss: 0.7508 - val_accuracy: 0.7228\n",
      "Epoch 14/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7328 - accuracy: 0.7188 - val_loss: 0.7112 - val_accuracy: 0.7353\n",
      "Epoch 15/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7223 - accuracy: 0.7245 - val_loss: 0.7789 - val_accuracy: 0.7096\n",
      "Epoch 16/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7224 - accuracy: 0.7244 - val_loss: 0.7713 - val_accuracy: 0.7129\n",
      "Epoch 17/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.7179 - accuracy: 0.7268 - val_loss: 1.0836 - val_accuracy: 0.5720\n",
      "Epoch 18/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6948 - accuracy: 0.7391 - val_loss: 0.7373 - val_accuracy: 0.7199\n",
      "Epoch 19/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.7382 - val_loss: 0.6804 - val_accuracy: 0.7437\n",
      "Epoch 20/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.7408 - val_loss: 0.8430 - val_accuracy: 0.6844\n",
      "Epoch 21/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.7468 - val_loss: 0.7134 - val_accuracy: 0.7246\n",
      "Epoch 22/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6695 - accuracy: 0.7465 - val_loss: 0.6360 - val_accuracy: 0.7624\n",
      "Epoch 23/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.7651 - val_loss: 0.7273 - val_accuracy: 0.7327\n",
      "Epoch 24/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6584 - accuracy: 0.7489 - val_loss: 0.6603 - val_accuracy: 0.7517\n",
      "Epoch 25/25\n",
      "171/171 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.7653 - val_loss: 0.7036 - val_accuracy: 0.7173\n",
      "107/107 [==============================] - 0s 897us/step\n"
     ]
    }
   ],
   "source": [
    "prop_train_images_flat = image_reshape(prop_train_images)\n",
    "prop_val_images_flat = image_reshape(prop_val_images)\n",
    "prop_test_images_flat = image_reshape(prop_test_images)\n",
    "\n",
    "fnn_p_history, fnn_p_predictions, fnn_p_test_accuracy = train_model(fnn_p,\n",
    "                                                                 prop_train_images_flat,\n",
    "                                                                 prop_train_labels,\n",
    "                                                                 prop_val_images_flat,\n",
    "                                                                 prop_val_labels,\n",
    "                                                                 prop_test_images_flat,\n",
    "                                                                 prop_test_labels, \n",
    "                                                                 epochs = 25)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
