{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa344e92-cd93-41b1-b490-5ae2db2de245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 22:17:09.810661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import imagesize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63fb38-d092-4954-b8c2-d7488d87c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basophil',\n",
       " 'neutrophil',\n",
       " 'ig',\n",
       " 'monocyte',\n",
       " 'eosinophil',\n",
       " 'erythroblast',\n",
       " 'lymphocyte',\n",
       " 'platelet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset folder has 8 different folders, which represent 8 different bloodcells we will be classifying\n",
    "# Open up folder names and remove folder names that are not bloodcell types\n",
    "\n",
    "# Original kaggle dataset has images stored in each of the 8 folders, so we made a folder that contained all the images\n",
    "# so that it is easier to convert the images to numpy arrays later on\n",
    "\n",
    "bloodcells = os.listdir(\"bloodcells_dataset\")\n",
    "bloodcells = [x for x in bloodcells if x not in ['.DS_Store', 'All_Images']]\n",
    "\n",
    "bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9498f58a-b13e-42fa-ba8e-3b262f5930a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>type</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA_689200.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA_883452.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA_382161.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA_175579.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA_775722.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>PLATELET_495918.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>PLATELET_897238.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>PLATELET_750430.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>PLATELET_810431.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17091</th>\n",
       "      <td>PLATELET_499850.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17092 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    images  type  width  height\n",
       "0            BA_689200.jpg     0    360     363\n",
       "1            BA_883452.jpg     0    360     363\n",
       "2            BA_382161.jpg     0    366     369\n",
       "3            BA_175579.jpg     0    360     363\n",
       "4            BA_775722.jpg     0    360     363\n",
       "...                    ...   ...    ...     ...\n",
       "17087  PLATELET_495918.jpg     7    360     363\n",
       "17088  PLATELET_897238.jpg     7    360     363\n",
       "17089  PLATELET_750430.jpg     7    360     363\n",
       "17090  PLATELET_810431.jpg     7    360     363\n",
       "17091  PLATELET_499850.jpg     7    360     363\n",
       "\n",
       "[17092 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize empty dataframe to store image strings and bloodcell type\n",
    "df = pd.DataFrame(np.nan, \n",
    "                  index = [0], \n",
    "                  columns = ['images', 'type'])\n",
    "\n",
    "# loop through bloodcell types and store image paths and bloodcell categories\n",
    "for i in range(len(bloodcells)):\n",
    "\n",
    "    images = os.listdir('bloodcells_dataset/' + bloodcells[i]) # jpg string paths\n",
    "    \n",
    "    images_df = pd.DataFrame(data = {'images': images, \n",
    "                                     'type': bloodcells[i]})\n",
    "    \n",
    "    df = pd.concat([df, images_df])\n",
    "\n",
    "# drop row that was first initialized with NaNs\n",
    "df = df.dropna(how = 'all')\n",
    "\n",
    "# Convert bloodcel types to numbers for our model\n",
    "le = LabelEncoder()\n",
    "df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "# Store dimensions of image incase we find different dimensions \n",
    "df['width'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[0])\n",
    "df['height'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[1])\n",
    "\n",
    "# Reset index and remove image paths that may have been accidentally copied\n",
    "df = df[df['images'].str.contains('copy') == False]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970a3e57-80ad-4791-8320-39692f45fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuals for count of height/width and count of each blood cell type\n",
    "# Maybe use function to plot\n",
    "\n",
    "# df[['height', 'width']].value_counts().reset_index(name = 'count') \n",
    "# df[['type']].value_counts().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46f4355-e4ed-423d-89d8-f4e31d2e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "\n",
    "    def __init__(self, data, sampling_method):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "    def sample_data(self, sampling_percent = 0.8):\n",
    "\n",
    "        df = self.data.copy()\n",
    "\n",
    "        category_counts = df[['type']].value_counts().reset_index(name = 'count')\n",
    "\n",
    "        train = pd.DataFrame(np.nan, index = [0], columns = list(df.columns))\n",
    "\n",
    "        if self.sampling_method == 'weighted':\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                if category_counts['count'][i] >= 2000:\n",
    "                    add_samples = df[df['type'] == type].sample(1500)\n",
    "                else: \n",
    "                    add_samples = df[df['type'] == type].sample(1000)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        elif self.sampling_method == 'proportional': \n",
    "\n",
    "            num_samples = int(sampling_percent * len(df))\n",
    "            \n",
    "            category_counts['prop'] = category_counts['count'] / len(df)\n",
    "            category_counts['prop_samples'] = category_counts['prop'] * num_samples\n",
    "            category_counts['prop_samples'] = category_counts['prop_samples'].astype('int32')\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                samples = category_counts['prop_samples'][i]\n",
    "            \n",
    "                add_samples = df[df['type'] == type].sample(samples)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        train = train.dropna(how = 'all')\n",
    "\n",
    "        float_cols = train.select_dtypes(np.number)\n",
    "\n",
    "        train[float_cols.columns] = float_cols.astype('int32')\n",
    "\n",
    "        test = df[~df['images'].isin(train['images'])]    \n",
    "\n",
    "        return train, test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f24ebb-9579-4255-9ec1-0988dc5aeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampling = Sampling(df, 'weighted')\n",
    "proportional_sampling = Sampling(df, 'proportional')\n",
    "\n",
    "weighted_train, weighted_test = weighted_sampling.sample_data()\n",
    "prop_train, prop_test = proportional_sampling.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68225d0e-9948-457f-ba14-dee6f4920d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Images:\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.file_names = (self.data)['images'].apply(lambda x: 'bloodcells_dataset/All_Images/' + x)\n",
    "\n",
    "        self.labels = self.data['type']\n",
    "\n",
    "    def load_image(self, file_name, resize):\n",
    "        \n",
    "        raw = tf.io.read_file(file_name)\n",
    "        \n",
    "        tensor = tf.io.decode_image(raw, expand_animations = False)\n",
    "        \n",
    "        tensor = tf.image.resize(tensor, size = [resize, resize])\n",
    "        \n",
    "        tensor = tf.cast(tensor, tf.float32) / 255.0\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def create_image_arrays(self, resize = 32):\n",
    "\n",
    "      file_names = self.file_names\n",
    "\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "        \n",
    "      dataset = dataset.map(lambda file_name: self.load_image(file_name, resize))\n",
    "        \n",
    "      images = np.array(list(dataset))\n",
    "        \n",
    "      return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbedfb7-47ac-4a9f-bb99-fabcd515af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 22:17:19.939783: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14.955910921096802"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "weighted_train_image_df, weighted_test_image_df = Convert_Images(weighted_train), Convert_Images(weighted_test)\n",
    "prop_train_image_df, prop_test_image_df = Convert_Images(prop_train), Convert_Images(prop_test)\n",
    "\n",
    "\n",
    "resize_pixels = 32\n",
    "\n",
    "weighted_train_images, weighted_train_labels = weighted_train_image_df.create_image_arrays(resize_pixels), weighted_train_image_df.labels\n",
    "weighted_test_images, weighted_test_labels = weighted_test_image_df.create_image_arrays(resize_pixels), weighted_test_image_df.labels\n",
    "prop_train_images, prop_train_labels = prop_train_image_df.create_image_arrays(resize_pixels), prop_train_image_df.labels\n",
    "prop_test_images, prop_test_labels = prop_test_image_df.create_image_arrays(resize_pixels), prop_test_image_df.labels\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8435b02e-1541-4ce4-b183-efca096a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data, test_labels, optimizer = 'adam', epochs = 5, batch_size = 64):\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, \n",
    "                        train_labels, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size)\n",
    "\n",
    "    predictions = (model.predict(test_data)).argmax(axis = 1)\n",
    "\n",
    "    test_accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "\n",
    "    return history, predictions, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed2467d-ce27-4f82-b321-a8968fb0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st of 3 models\n",
    "# Simple model\n",
    "# One for weighted sampling and proportional sampling\n",
    "\n",
    "# Simple Model - Weighted Sampling\n",
    "sm_w = models.Sequential(\n",
    "    \n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (resize_pixels, resize_pixels, 3)),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "\n",
    "        # flatten into 1d array\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Neural network\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "\n",
    "        layers.Dropout(rate = 0.2),\n",
    "        \n",
    "        # 8 different categories\n",
    "        layers.Dense(8) \n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "# Simple Model - Proportional Sampling\n",
    "sm_p = models.clone_model(sm_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9e053d-f9db-408d-9f45-48b4403ce54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 1.4084 - accuracy: 0.4698\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 6s 36ms/step - loss: 0.8723 - accuracy: 0.6805\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.7531 - accuracy: 0.7201\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.6872 - accuracy: 0.7490\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.6213 - accuracy: 0.7661\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.5786 - accuracy: 0.7828\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.5226 - accuracy: 0.8065\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.4800 - accuracy: 0.8238\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.4613 - accuracy: 0.8300\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.4329 - accuracy: 0.8426\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.4109 - accuracy: 0.8519\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.3876 - accuracy: 0.8609\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.3655 - accuracy: 0.8689\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.3435 - accuracy: 0.8745\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.3237 - accuracy: 0.8869\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.3127 - accuracy: 0.8892\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 5s 34ms/step - loss: 0.2948 - accuracy: 0.8902\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.2728 - accuracy: 0.9035\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.2827 - accuracy: 0.9014\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.2580 - accuracy: 0.9092\n",
      "222/222 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "sm_w_history, sm_w_predictions, sm_w_test_accuracy = train_model(sm_w,\n",
    "                                                                 weighted_train_images,\n",
    "                                                                 weighted_train_labels,\n",
    "                                                                 weighted_test_images,\n",
    "                                                                 weighted_test_labels,\n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b30932-2847-44c0-8672-b33fb28e5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "214/214 [==============================] - 7s 33ms/step - loss: 1.3536 - accuracy: 0.4979\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 7s 31ms/step - loss: 0.8594 - accuracy: 0.6870\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 7s 31ms/step - loss: 0.7359 - accuracy: 0.7293\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 7s 34ms/step - loss: 0.6723 - accuracy: 0.7506\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 7s 30ms/step - loss: 0.6381 - accuracy: 0.7603\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 7s 31ms/step - loss: 0.5561 - accuracy: 0.7936\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 6s 30ms/step - loss: 0.4974 - accuracy: 0.8107\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 7s 31ms/step - loss: 0.4590 - accuracy: 0.8303\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.4300 - accuracy: 0.8414\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 9s 44ms/step - loss: 0.4061 - accuracy: 0.8488\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 5s 26ms/step - loss: 0.3715 - accuracy: 0.8674\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.3579 - accuracy: 0.8704\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.3354 - accuracy: 0.8802\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 6s 30ms/step - loss: 0.3162 - accuracy: 0.8858\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 7s 30ms/step - loss: 0.3104 - accuracy: 0.8867\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.3005 - accuracy: 0.8942\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 7s 33ms/step - loss: 0.2848 - accuracy: 0.8991\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.2820 - accuracy: 0.9004\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 7s 33ms/step - loss: 0.2637 - accuracy: 0.9051\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 7s 32ms/step - loss: 0.2530 - accuracy: 0.9116\n",
      "107/107 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "sm_p_history, sm_p_predictions, sm_p_test_accuracy = train_model(sm_p,\n",
    "                                                                 prop_train_images,\n",
    "                                                                 prop_train_labels,\n",
    "                                                                 prop_test_images,\n",
    "                                                                 prop_test_labels, \n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc3e39c-3ac7-4be4-8499-1f019e3fb762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8864918217710096, 0.9085597429155712)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_w_test_accuracy, sm_p_test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
