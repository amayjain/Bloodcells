{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa344e92-cd93-41b1-b490-5ae2db2de245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 22:36:35.273541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import imagesize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63fb38-d092-4954-b8c2-d7488d87c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basophil',\n",
       " 'neutrophil',\n",
       " 'ig',\n",
       " 'monocyte',\n",
       " 'eosinophil',\n",
       " 'erythroblast',\n",
       " 'lymphocyte',\n",
       " 'platelet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset folder has 8 different folders, which represent 8 different bloodcells we will be classifying\n",
    "# Open up folder names and remove folder names that are not bloodcell types\n",
    "\n",
    "# Original kaggle dataset has images stored in each of the 8 folders, so we made a folder that contained all the images\n",
    "# so that it is easier to convert the images to numpy arrays later on\n",
    "\n",
    "bloodcells = os.listdir(\"bloodcells_dataset\")\n",
    "bloodcells = [x for x in bloodcells if x not in ['.DS_Store', 'All_Images']]\n",
    "\n",
    "bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498f58a-b13e-42fa-ba8e-3b262f5930a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty dataframe to store image strings and bloodcell type\n",
    "df = pd.DataFrame(np.nan, \n",
    "                  index = [0], \n",
    "                  columns = ['images', 'type'])\n",
    "\n",
    "# loop through bloodcell types and store image paths and bloodcell categories\n",
    "for i in range(len(bloodcells)):\n",
    "\n",
    "    images = os.listdir('bloodcells_dataset/' + bloodcells[i]) # jpg string paths\n",
    "    \n",
    "    images_df = pd.DataFrame(data = {'images': images, \n",
    "                                     'type': bloodcells[i]})\n",
    "    \n",
    "    df = pd.concat([df, images_df])\n",
    "\n",
    "# drop row that was first initialized with NaNs\n",
    "df = df.dropna(how = 'all')\n",
    "\n",
    "# Convert bloodcel types to numbers for our model\n",
    "le = LabelEncoder()\n",
    "df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "# Store dimensions of image incase we find different dimensions \n",
    "df['width'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[0])\n",
    "df['height'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[1])\n",
    "\n",
    "# Reset index and remove image paths that may have been accidentally copied\n",
    "df = df[df['images'].str.contains('copy') == False]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a3e57-80ad-4791-8320-39692f45fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuals for count of height/width and count of each blood cell type\n",
    "# Maybe use function to plot\n",
    "\n",
    "# df[['height', 'width']].value_counts().reset_index(name = 'count') \n",
    "# df[['type']].value_counts().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f4355-e4ed-423d-89d8-f4e31d2e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "\n",
    "    def __init__(self, data, sampling_method):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "    def sample_data(self, sampling_percent = 0.8):\n",
    "\n",
    "        df = self.data.copy()\n",
    "\n",
    "        category_counts = df[['type']].value_counts().reset_index(name = 'count')\n",
    "\n",
    "        train = pd.DataFrame(np.nan, index = [0], columns = list(df.columns))\n",
    "\n",
    "        if self.sampling_method == 'weighted':\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                if category_counts['count'][i] >= 2000:\n",
    "                    add_samples = df[df['type'] == type].sample(1500)\n",
    "                else: \n",
    "                    add_samples = df[df['type'] == type].sample(1000)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        elif self.sampling_method == 'proportional': \n",
    "\n",
    "            num_samples = int(sampling_percent * len(df))\n",
    "            \n",
    "            category_counts['prop'] = category_counts['count'] / len(df)\n",
    "            category_counts['prop_samples'] = category_counts['prop'] * num_samples\n",
    "            category_counts['prop_samples'] = category_counts['prop_samples'].astype('int32')\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                samples = category_counts['prop_samples'][i]\n",
    "            \n",
    "                add_samples = df[df['type'] == type].sample(samples)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        train = train.dropna(how = 'all')\n",
    "\n",
    "        float_cols = train.select_dtypes(np.number)\n",
    "\n",
    "        train[float_cols.columns] = float_cols.astype('int32')\n",
    "\n",
    "        test = df[~df['images'].isin(train['images'])]    \n",
    "\n",
    "        return train, test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f24ebb-9579-4255-9ec1-0988dc5aeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampling = Sampling(df, 'weighted')\n",
    "proportional_sampling = Sampling(df, 'proportional')\n",
    "\n",
    "weighted_train, weighted_test = weighted_sampling.sample_data()\n",
    "prop_train, prop_test = proportional_sampling.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68225d0e-9948-457f-ba14-dee6f4920d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Images:\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        self.file_names = (self.data)['images'].apply(lambda x: 'bloodcells_dataset/All_Images/' + x)\n",
    "\n",
    "        self.labels = self.data['type']\n",
    "\n",
    "    def load_image(self, file_name, resize):\n",
    "        \n",
    "        raw = tf.io.read_file(file_name)\n",
    "        \n",
    "        tensor = tf.io.decode_image(raw, expand_animations = False)\n",
    "        \n",
    "        tensor = tf.image.resize(tensor, size = [resize, resize])\n",
    "        \n",
    "        tensor = tf.cast(tensor, tf.float32) / 255.0\n",
    "        \n",
    "        return tensor\n",
    "\n",
    "    def image_arrays_and_labels(self, resize = 32):\n",
    "\n",
    "      file_names = self.file_names\n",
    "\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(file_names)\n",
    "        \n",
    "      dataset = dataset.map(lambda file_name: self.load_image(file_name, resize))\n",
    "        \n",
    "      images = np.array(list(dataset))\n",
    "        \n",
    "      return images, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbedfb7-47ac-4a9f-bb99-fabcd515af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_train_image_df, weighted_test_image_df = Convert_Images(weighted_train), Convert_Images(weighted_test)\n",
    "prop_train_image_df, prop_test_image_df = Convert_Images(prop_train), Convert_Images(prop_test)\n",
    "\n",
    "\n",
    "resize_pixels = 32\n",
    "\n",
    "weighted_train_images, weighted_train_labels = weighted_train_image_df.image_arrays_and_labels(resize_pixels)\n",
    "weighted_test_images, weighted_test_labels = weighted_test_image_df.image_arrays_and_labels(resize_pixels)\n",
    "prop_train_images, prop_train_labels = prop_train_image_df.image_arrays_and_labels(resize_pixels)\n",
    "prop_test_images, prop_test_labels = prop_test_image_df.image_arrays_and_labels(resize_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435b02e-1541-4ce4-b183-efca096a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data, test_labels, optimizer = 'adam', epochs = 5, batch_size = 64):\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, \n",
    "                        train_labels, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size)\n",
    "\n",
    "    predictions = (model.predict(test_data)).argmax(axis = 1)\n",
    "\n",
    "    test_accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "\n",
    "    return history, predictions, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2467d-ce27-4f82-b321-a8968fb0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st of 3 models\n",
    "# Simple model\n",
    "# One for weighted sampling and proportional sampling\n",
    "\n",
    "# Simple Model - Weighted Sampling\n",
    "sm_w = models.Sequential(\n",
    "    \n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (resize_pixels, resize_pixels, 3)),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "\n",
    "        # flatten into 1d array\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Neural network\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "\n",
    "        layers.Dropout(rate = 0.2),\n",
    "        \n",
    "        # 8 different categories\n",
    "        layers.Dense(8) \n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "# Simple Model - Proportional Sampling\n",
    "sm_p = models.clone_model(sm_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9e053d-f9db-408d-9f45-48b4403ce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_w_history, sm_w_predictions, sm_w_test_accuracy = train_model(sm_w,\n",
    "                                                                 weighted_train_images,\n",
    "                                                                 weighted_train_labels,\n",
    "                                                                 weighted_test_images,\n",
    "                                                                 weighted_test_labels,\n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b30932-2847-44c0-8672-b33fb28e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_p_history, sm_p_predictions, sm_p_test_accuracy = train_model(sm_p,\n",
    "                                                                 prop_train_images,\n",
    "                                                                 prop_train_labels,\n",
    "                                                                 prop_test_images,\n",
    "                                                                 prop_test_labels, \n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3e39c-3ac7-4be4-8499-1f019e3fb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_w_test_accuracy, sm_p_test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
