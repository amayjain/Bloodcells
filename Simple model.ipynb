{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa344e92-cd93-41b1-b490-5ae2db2de245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 20:57:38.172865: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import imagesize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad63fb38-d092-4954-b8c2-d7488d87c56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basophil',\n",
       " 'neutrophil',\n",
       " 'ig',\n",
       " 'monocyte',\n",
       " 'eosinophil',\n",
       " 'erythroblast',\n",
       " 'lymphocyte',\n",
       " 'platelet']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloodcells = os.listdir(\"bloodcells_dataset\")\n",
    "bloodcells = [x for x in bloodcells if x not in ['.DS_Store', 'All_Images']]\n",
    "\n",
    "bloodcells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9498f58a-b13e-42fa-ba8e-3b262f5930a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>type</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA_689200.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA_883452.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BA_382161.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA_175579.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BA_775722.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17087</th>\n",
       "      <td>PLATELET_495918.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17088</th>\n",
       "      <td>PLATELET_897238.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17089</th>\n",
       "      <td>PLATELET_750430.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17090</th>\n",
       "      <td>PLATELET_810431.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17091</th>\n",
       "      <td>PLATELET_499850.jpg</td>\n",
       "      <td>7</td>\n",
       "      <td>360</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17092 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    images  type  width  height\n",
       "0            BA_689200.jpg     0    360     363\n",
       "1            BA_883452.jpg     0    360     363\n",
       "2            BA_382161.jpg     0    366     369\n",
       "3            BA_175579.jpg     0    360     363\n",
       "4            BA_775722.jpg     0    360     363\n",
       "...                    ...   ...    ...     ...\n",
       "17087  PLATELET_495918.jpg     7    360     363\n",
       "17088  PLATELET_897238.jpg     7    360     363\n",
       "17089  PLATELET_750430.jpg     7    360     363\n",
       "17090  PLATELET_810431.jpg     7    360     363\n",
       "17091  PLATELET_499850.jpg     7    360     363\n",
       "\n",
       "[17092 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = {'images': [np.nan], 'type': [np.nan]})\n",
    "\n",
    "for i in range(len(bloodcells)):\n",
    "\n",
    "    images = os.listdir('bloodcells_dataset/' + bloodcells[i]) # jpg string paths\n",
    "    \n",
    "    images_df = pd.DataFrame(data = {'images': images, 'type': bloodcells[i]})\n",
    "    \n",
    "    df = pd.concat([df, images_df])\n",
    "\n",
    "df = df.dropna(how = 'all')\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['type'] = le.fit_transform(df['type'])\n",
    "\n",
    "df['width'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[0])\n",
    "df['height'] = df['images'].apply(lambda x: imagesize.get('bloodcells_dataset/All_Images/' + x)[1])\n",
    "\n",
    "df = df[df['images'].str.contains('copy') == False]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970a3e57-80ad-4791-8320-39692f45fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuals for count of height/width and count of each blood cell type\n",
    "# Maybe use function to plot\n",
    "\n",
    "# df[['height', 'width']].value_counts().reset_index(name = 'count') \n",
    "# df[['type']].value_counts().reset_index(name = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46f4355-e4ed-423d-89d8-f4e31d2e5b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling:\n",
    "\n",
    "    def __init__(self, data, sampling_method):\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "    def sample_data(self, sampling_percent = 0.8):\n",
    "\n",
    "        df = self.data.copy()\n",
    "\n",
    "        category_counts = df[['type']].value_counts().reset_index(name = 'count')\n",
    "\n",
    "        train = pd.DataFrame(np.nan, index = [0], columns = list(df.columns))\n",
    "\n",
    "        if self.sampling_method == 'weighted':\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                if category_counts['count'][i] >= 2000:\n",
    "                    add_samples = df[df['type'] == type].sample(1500)\n",
    "                else: \n",
    "                    add_samples = df[df['type'] == type].sample(1000)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        elif self.sampling_method == 'proportional': \n",
    "\n",
    "            num_samples = int(sampling_percent * len(df))\n",
    "            \n",
    "            category_counts['prop'] = category_counts['count'] / len(df)\n",
    "            category_counts['prop_samples'] = category_counts['prop'] * num_samples\n",
    "            category_counts['prop_samples'] = category_counts['prop_samples'].astype('int32')\n",
    "\n",
    "            for i in range(len(category_counts)):\n",
    "\n",
    "                type = category_counts['type'][i]\n",
    "            \n",
    "                samples = category_counts['prop_samples'][i]\n",
    "            \n",
    "                add_samples = df[df['type'] == type].sample(samples)\n",
    "            \n",
    "                train = pd.concat([train, add_samples])\n",
    "\n",
    "        train = train.dropna(how = 'all')\n",
    "\n",
    "        float_cols = train.select_dtypes(np.number)\n",
    "\n",
    "        train[float_cols.columns] = float_cols.astype('int32')\n",
    "\n",
    "        test = df[~df['images'].isin(train['images'])]    \n",
    "\n",
    "        return train, test\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f24ebb-9579-4255-9ec1-0988dc5aeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampling = Sampling(df, 'weighted')\n",
    "proportional_sampling = Sampling(df, 'proportional')\n",
    "\n",
    "weighted_train, weighted_test = weighted_sampling.sample_data()\n",
    "prop_train, prop_test = proportional_sampling.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68225d0e-9948-457f-ba14-dee6f4920d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convert_Images:\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def convert(self, resize):\n",
    "\n",
    "        df = self.data.copy()\n",
    "\n",
    "        resize_pixels = (resize, resize)\n",
    "\n",
    "        images = np.array([np.array(Image.open('bloodcells_dataset/All_Images/' + image).resize(resize_pixels)) for image in df['images']])\n",
    "\n",
    "        images = tf.convert_to_tensor(images / 255.0)\n",
    "\n",
    "        labels = tf.convert_to_tensor(df['type'])\n",
    "\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fbedfb7-47ac-4a9f-bb99-fabcd515af70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 20:58:16.876651: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.01899695396423"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "weighted_train_image_df, weighted_test_image_df = Convert_Images(weighted_train), Convert_Images(weighted_test)\n",
    "prop_train_image_df, prop_test_image_df = Convert_Images(prop_train), Convert_Images(prop_test)\n",
    "\n",
    "\n",
    "resize_pixels = 32\n",
    "\n",
    "weighted_train_images, weighted_train_labels = weighted_train_image_df.convert(resize_pixels)\n",
    "weighted_test_images, weighted_test_labels = weighted_test_image_df.convert(resize_pixels)\n",
    "prop_train_images, prop_train_labels = prop_train_image_df.convert(resize_pixels)\n",
    "prop_test_images, prop_test_labels = prop_test_image_df.convert(resize_pixels)\n",
    "\n",
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c400e9-3c3d-4b12-982d-1d837b06fcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening an image\n",
    "# test_img = Image.open('bloodcells_dataset/All_Images/' + df['images'][14000])\n",
    "\n",
    "# Resize\n",
    "# test_img.resize((320, 320))\n",
    "\n",
    "# Crop\n",
    "# test_img.crop((20, 20, 340, 340))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8435b02e-1541-4ce4-b183-efca096a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, train_labels, test_data, test_labels, optimizer = 'adam', epochs = 5, batch_size = 64):\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "    history = model.fit(train_data, \n",
    "                        train_labels, \n",
    "                        epochs = epochs, \n",
    "                        batch_size = batch_size)\n",
    "\n",
    "    predictions = (model.predict(test_data)).argmax(axis = 1)\n",
    "\n",
    "    test_accuracy = np.sum(predictions == test_labels) / len(test_labels)\n",
    "\n",
    "    return history, predictions, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed2467d-ce27-4f82-b321-a8968fb0e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st of 3 models\n",
    "# Simple model\n",
    "# One for weighted sampling and proportional sampling\n",
    "\n",
    "# Simple Model - Weighted Sampling\n",
    "sm_w = models.Sequential(\n",
    "    \n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (resize_pixels, resize_pixels, 3)),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(32, (3, 3), activation = 'relu'),\n",
    "        \n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation = 'relu'),\n",
    "\n",
    "        # flatten into 1d array\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # Neural network\n",
    "        layers.Dense(64, activation = 'relu'),\n",
    "\n",
    "        layers.Dropout(rate = 0.2),\n",
    "        \n",
    "        # 8 different categories\n",
    "        layers.Dense(8) \n",
    "    ]\n",
    "    \n",
    ")\n",
    "\n",
    "# Simple Model - Proportional Sampling\n",
    "sm_p = models.clone_model(sm_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d9e053d-f9db-408d-9f45-48b4403ce54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "157/157 [==============================] - 5s 29ms/step - loss: 1.4757 - accuracy: 0.4352\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.9022 - accuracy: 0.6573\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 6s 39ms/step - loss: 0.7530 - accuracy: 0.7174\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.6512 - accuracy: 0.7605\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.5969 - accuracy: 0.7830\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 6s 35ms/step - loss: 0.5369 - accuracy: 0.8049\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 0.4989 - accuracy: 0.8138\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.4877 - accuracy: 0.8222\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.4512 - accuracy: 0.8395\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.4210 - accuracy: 0.8477\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 5s 30ms/step - loss: 0.3783 - accuracy: 0.8630\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 0.3657 - accuracy: 0.8664\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 13s 83ms/step - loss: 0.3645 - accuracy: 0.8670\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 6s 40ms/step - loss: 0.3255 - accuracy: 0.8804\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 6s 40ms/step - loss: 0.3214 - accuracy: 0.8849\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 6s 37ms/step - loss: 0.3027 - accuracy: 0.8922\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 0.2818 - accuracy: 0.8978\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 5s 31ms/step - loss: 0.2693 - accuracy: 0.9041\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2635 - accuracy: 0.9046\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 5s 32ms/step - loss: 0.2543 - accuracy: 0.9075\n",
      "222/222 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "sm_w_history, sm_w_predictions, sm_w_test_accuracy = train_model(sm_w,\n",
    "                                                                 weighted_train_images,\n",
    "                                                                 weighted_train_labels,\n",
    "                                                                 weighted_test_images,\n",
    "                                                                 weighted_test_labels,\n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b30932-2847-44c0-8672-b33fb28e5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 72/214 [=========>....................] - ETA: 9s - loss: 1.7062 - accuracy: 0.3618"
     ]
    }
   ],
   "source": [
    "sm_p_history, sm_p_predictions, sm_p_test_accuracy = train_model(sm_p,\n",
    "                                                                 prop_train_images,\n",
    "                                                                 prop_train_labels,\n",
    "                                                                 prop_test_images,\n",
    "                                                                 prop_test_labels, \n",
    "                                                                 epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3e39c-3ac7-4be4-8499-1f019e3fb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_w_test_accuracy, sm_p_test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
